<chapter id="using" xreflabel="Using the Condor Roll">


<title>Using the Condor Roll</title>

<section id="using-condor" xreflabel="Using Condor">
<title>Using Condor</title>

<para>

This section explains the Condor pool configuration on Rocks clusters,
and provides link to further documentation.

</para>

<para>
	
Machines in the Condor pool can serve a variety of roles:
<itemizedlist>

<listitem>
<para>
<emphasis>Central manager</emphasis> machine is the collector of 
information, and the negotiator between resources and resource requests. 
In any Condor pool one machine must be the Central Manager.
</para>
</listitem>

<listitem>
<para>
<emphasis>Submit machine</emphasis> allows Condor jobs submission. 
</para>
</listitem>

<listitem>
<para>
<emphasis>Execute machine</emphasis> provides resources for Condor jobs execution.
</para>
</listitem>

</itemizedlist>

</para>

<para>
The Frontend is configured as the pool's Central Manager, and the Submit machine.  
The rest of the nodes are configured as Submit/Execute
machines. 
</para>

<para>
The configuration of condor is done during the install, the resulting 
configuration files are located in /opt/condor/etc. To reconfigure condor pool
use /opt/condor/sbin/CondorConf command, and then restart condor daemons so 
the new configuration takes an effect. CondorConf takes <emphasis>-h</emphasis> flag
that gives the usage explanation on stdout. Usually, the reconfiguration is only needed
when you have multiple network interfaces on your nodes, and the default configuration
is not using your desired interface.
</para>

<para>
By default, on the frontend condor daemons are running on <emphasis>eth1</emphasis>
interface, and on the compute nodes on <emphasis>eth0</emphasis> interface.
</para>

<para>
Here is en axample command for configuring condor as the pool manager and the submit
machine on the frontend, assuming the forntend has IP 198.202.89.155:
</para>

<screen>
/opt/condor/sbin/CondorConf -n frontend -t sm -m 198.202.89.155
</screen>

<para>
The following command will configure compute nodes as Submit/Execute machines:
</para>

<screen>
/opt/condor/sbin/CondorConf -n compute -t se -m 198.202.89.155
</screen>

<para>
To find information about administrering and using Condor Pools
please see the original Condor manual at <ulink
url="http://www.cs.wisc.edu/condor/manual">Condor
manuals</ulink>
</para>

</section>

<section id="testing-condor" xreflabel="Testing Condor">
<title>Testing the Condor Roll</title>

<orderedlist>

<listitem>
<para>
First, make sure condor daemons are running by executing:
</para>

<screen>
# ps -ef | grep condor
</screen>

<para>
On the frontend, the output should be similar to following:
</para>

<screen>
condor    2623     1  0 Apr19 ?        00:04:26 /opt/condor/sbin/condor_master
condor    2646  2623  0 Apr19 ?        00:20:25 condor_collector -f
condor    2647  2623  0 Apr19 ?        00:04:56 condor_negotiator -f
condor    2649  2623  0 Apr19 ?        00:00:02 condor_schedd -f
</screen>

<para>
And on the compute nodes, the output should be similar to following:
</para>

<screen>
condor   17007     1  0 Apr19 ?        00:01:09 /opt/condor/sbin/condor_master
condor   17009 17007  0 Apr19 ?        00:00:02 condor_schedd -f
condor   17010 17007  0 Apr19 ?        00:09:09 condor_startd -f
</screen>
</listitem>


<listitem>
<para>
Try a test job submission.
If you don't already have a non-root user account, you'll need to create one:
</para>

<screen>
# useradd testuser
# rocks sync users
</screen>

<para>
Then, to run a test job submission, execute:
</para>

<screen>
# su - testuser 
$ cp /opt/condor/tests/hello.* .
$ condor_submit hello.sub
</screen>
</listitem>

<listitem>
<para>
Check if jobs are submitted by executing:
</para>

<screen>
$ condor_q
</screen>

<para>
The output should be similar to:
</para>

<screen>
-- Submitter: jeebs.rocksclusters.org : <172.19.119.241:44626> : jeebs.rocksclusters.org
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   testuser       12/12 09:38   0+00:00:01 R  0   0.0  hello.sh          

1 jobs; 0 idle, 1 running, 0 held
</screen>

<para>
<emphasis>R</emphasis> in status column(ST) means running.
<emphasis>I</emphasis> means idling.
</para>

</listitem>


<listitem>
<para>
Once the queue is empty (above command shows no jobs) can see the history of
jobs execution with:
</para>

<screen>
$ condor_history
</screen>

<para>
To see all the nodes in the condor pool do:
</para>

<screen>
$ condor_status
</screen>

<para>
The output should be similar to:
</para>

<screen>
Name          OpSys       Arch   State      Activity   LoadAv Mem   ActvtyTime

vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:35:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:25:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:05
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:06
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:07
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:08
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:10:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:07
vm1@compute-0 LINUX       INTEL  Owner      Idle       0.860   506  0+00:00:09
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:07

                     Machines Owner Claimed Unclaimed Matched Preempting

         INTEL/LINUX       28     1       0        27       0          0

               Total       28     1       0        27       0          0
</screen>

</listitem>

</orderedlist>


</section>

</chapter>
