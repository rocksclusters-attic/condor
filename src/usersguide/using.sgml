<chapter id="using" xreflabel="Using the Condor Roll">


<title>Using the Condor Roll</title>

<section id="using-condor" xreflabel="Using Condor">
<title>Using Condor</title>

<para>

This section explains the Condor pool configuration on Rocks clusters,
and provides link to further documentation.

</para>

<para>
	
Machines in the Condor pool can serve a variety of roles:
<itemizedlist>

<listitem>
<para>
<emphasis>Central manager</emphasis> machine is the collector of 
information, and the negotiator between resources and resource requests. 
In any Condor pool one machine must be the Central Manager.
</para>
</listitem>

<listitem>
<para>
<emphasis>Submit machine</emphasis> allows Condor jobs submission. 
</para>
</listitem>

<listitem>
<para>
<emphasis>Execute machine</emphasis> provides resources for Condor jobs execution.
</para>
</listitem>

</itemizedlist>

</para>

<para>
The Frontend is configured as the pool's Central Manager, and the Submit machine.  
The rest of the nodes are configured as Submit/Execute
machines. 
</para>

<para>
The configuration of condor is done during the install, the resulting 
configuration files are located in /opt/condor/etc. To reconfigure condor pool
use /opt/condor/sbin/CondorConf command, and then restart condor daemons so 
the new configuration takes an effect. CondorConf takes <emphasis>-h</emphasis> flag
that gives the usage explanation on stdout. Usually, the reconfiguration is only needed
when you have multiple network interfaces on your nodes, and the default configuration
is not using your desired interface.
</para>

<para>
By default, on the frontend condor daemons are running on <emphasis>eth1</emphasis>
interface, and on the compute nodes on <emphasis>eth0</emphasis> interface.
</para>

<para>
Here is en axample command for configuring condor as the pool manager and the submit
machine on the frontend, assuming the forntend has IP 198.202.89.155:
</para>

<screen>
/opt/condor/sbin/CondorConf -n frontend -t sm -m 198.202.89.155
</screen>

<para>
The following command will configure compute nodes as Submit/Execute machines:
</para>

<screen>
/opt/condor/sbin/CondorConf -n compute -t se -m 198.202.89.155
</screen>

<para>
To find information about administrering and using Condor Pools
please see the original Condor manual at <ulink
url="http://www.cs.wisc.edu/condor/manual">Condor
manuals</ulink>
</para>

</section>

<section id="testing-condor" xreflabel="Testing Condor">
<title>Testing the Condor Roll</title>

<orderedlist>

<listitem>
<para>
First, make sure condor daemons are running by executing:
</para>

<screen>
# ps -ef | grep condor
</screen>

<para>
On the frontend, the output should be similar to following:
</para>

<screen>
condor    2623     1  0 Apr19 ?        00:04:26 /opt/condor/sbin/condor_master
condor    2646  2623  0 Apr19 ?        00:20:25 condor_collector -f
condor    2647  2623  0 Apr19 ?        00:04:56 condor_negotiator -f
condor    2649  2623  0 Apr19 ?        00:00:02 condor_schedd -f
</screen>

<para>
And on the compute nodes, the output should be similar to following:
</para>

<screen>
condor   17007     1  0 Apr19 ?        00:01:09 /opt/condor/sbin/condor_master
condor   17009 17007  0 Apr19 ?        00:00:02 condor_schedd -f
condor   17010 17007  0 Apr19 ?        00:09:09 condor_startd -f
</screen>
</listitem>


<listitem>
<para>
Try a test job submission.
</para>

<screen>
# su - condor 
$ cd ~condor/tests
$ condor_submit subs/hmmpfam3
</screen>
</listitem>

<listitem>
<para>
Check if jobs are submitted by executing:
</para>

<screen>
$ condor_q
</screen>

<para>
The output should be similar to:
</para>

<screen>
-- Submitter: rocks-155.sdsc.edu : <198.202.88.155:47289> : rocks-155.sdsc.edu
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   condor          4/27 23:02   0+00:00:48 R  0   0.2  hmmpfam data/db100
   1.1   condor          4/27 23:02   0+00:00:46 R  0   0.2  hmmpfam data/db100
   1.2   condor          4/27 23:02   0+00:00:44 R  0   0.2  hmmpfam data/db100
   1.3   condor          4/27 23:02   0+00:00:42 R  0   0.2  hmmpfam data/db100
   1.4   condor          4/27 23:02   0+00:00:38 R  0   0.2  hmmpfam data/db100
   1.5   condor          4/27 23:02   0+00:00:36 R  0   0.2  hmmpfam data/db100
   1.6   condor          4/27 23:02   0+00:00:34 R  0   0.2  hmmpfam data/db100
   1.7   condor          4/27 23:02   0+00:00:40 R  0   0.2  hmmpfam data/db100
   1.8   condor          4/27 23:02   0+00:00:32 I  0   0.2  hmmpfam data/db100
   1.9   condor          4/27 23:02   0+00:00:30 I  0   0.2  hmmpfam data/db100
</screen>

<para>
<emphasis>R</emphasis> in status column(ST) means running.
<emphasis>I</emphasis> means idling.
The output from the jobs will be in <emphasis>results/</emphasis>
</para>

</listitem>


<listitem>
<para>
Once the queue is empty (above command shows no jobs) can see the history of
jobs execution with:
</para>

<screen>
$ condor_history
</screen>

<para>
To see all the nodes in the condor pool do:
</para>

<screen>
$ condor_status
</screen>

<para>
The output should be similar to:
</para>

<screen>
Name          OpSys       Arch   State      Activity   LoadAv Mem   ActvtyTime

vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:45:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:35:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:40:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:25:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:30:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:05
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:06
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:07
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:20:08
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:10:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:15:07
vm1@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:04
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:05:07
vm1@compute-0 LINUX       INTEL  Owner      Idle       0.860   506  0+00:00:09
vm2@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:05
vm3@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:06
vm4@compute-0 LINUX       INTEL  Unclaimed  Idle       0.000   506  0+00:00:07

                     Machines Owner Claimed Unclaimed Matched Preempting

         INTEL/LINUX       28     1       0        27       0          0

               Total       28     1       0        27       0          0
</screen>

</listitem>

<listitem>
<para>
The directory ~condor/tests has a few tests programs with the corresponding job submit files
for running test jobs in different condor universes.  The test programs are in bin/, and the submit 
files are in subs/. The output of the jobs, if any, goes to results/. To run these tests as condor
simply execute condor_submit command followed by the desired submit file name from subs/.
For example:
</para>

<screen>
$ cd ~/tests
$ condor_submit subs/hmmpfam3
</screen>

<note>
<para>
The test program tests/bin/simple.mpi and its submit file test/subs/submit_mpi are provided
only as a reference. Current condor binaries do not work with MPI programs compiled with mpicc
version higher then v.1.2.4. If you wish to run jobs in MPI universe your programs should be
compiled with MPI versions 1.2.2 through 1.2.4. 
</para>
</note>

</listitem>

</orderedlist>


</section>

</chapter>
